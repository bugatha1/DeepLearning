# -*- coding: utf-8 -*-
"""05_tensorflow_trasfer_learning_fine_tuning_video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GzxTM8ClEl_AjHDIz64t730QcnByC9Iv
"""

!nvidia-smi

!wget https://raw.githubusercontent.com/bugatha1/tensorflow-deep-learning/main/helper_functions.py

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip

from helper_functions import unzip_data

unzip_data('10_food_classes_10_percent.zip')

import os

def walk_through_dir(dir_path):
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"there are {len(dirnames)} directories and {len(filenames)} filenames in {dirpath}")

walk_through_dir('10_food_classes_10_percent')

train_dir = '10_food_classes_10_percent/train'
test_dir = '10_food_classes_10_percent/test'

import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                                 image_size = IMG_SIZE,
                                                                 batch_size= BATCH_SIZE,
                                                                 label_mode = 'categorical')

test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                                image_size = IMG_SIZE,
                                                                batch_size = BATCH_SIZE,
                                                                label_mode = 'categorical')

train_data.class_names

for images, labels in train_data.take(1):
  print(images, labels)

import datetime

def create_tensorboard_callback(dir_name, experiment_name):
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)

  print(f"saving tensorboard log files into {log_dir}")
  return tensorboard_callback



base_model = tf.keras.applications.EfficientNetB0(include_top=False)

base_model.trainable = False

inputs = tf.keras.layers.Input(shape=(224, 224, 3), name='input_layer')

x = base_model(inputs)

print(f"shape after passing inputs to the model {x.shape}")

x = tf.keras.layers.GlobalAveragePooling2D(name='global_average_pooling_layer')(x)

print(f"shaper after global average pooling 2D {x.shape}")

outputs = tf.keras.layers.Dense(10, activation='softmax', name='output_layer')(x)

model_0 = tf.keras.Model(inputs, outputs)

model_0.compile(loss='categorical_crossentropy',
                optimizer = tf.keras.optimizers.Adam(),
                metrics = ['accuracy'])

history_10_percent = model_0.fit(train_data,
                                 epochs = 5,
                                 steps_per_epoch = len(train_data),
                                 validation_data = test_data,
                                 validation_steps = int(0.25* len(test_data)),
                                 callbacks = create_tensorboard_callback(dir_name='transfer_learning',
                                                                         experiment_name = '10_percent_feature_extraction'))

model_0.evaluate(test_data)

for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name)

base_model.summary()

model_0.summary()

# Plot the validation and training data separately
import matplotlib.pyplot as plt

def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  Args:
    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)
  """ 
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

plot_loss_curves(history_10_percent)